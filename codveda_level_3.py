# -*- coding: utf-8 -*-
"""Codveda Level-3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lqalh2bIbnN3TLcearocXb-K9TG5OtFX
"""

from google.colab import files
uploaded = files.upload()
for filename in uploaded.keys():
    print(f'Uploaded file: {filename} ({len(uploaded[filename])} bytes)')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
#Load the data
df = pd.read_csv('2) Stock Prices Data Set.csv')
print(df)

df['next_close'] = df.groupby('symbol')['close'].shift(-1)
print(df['next_close'])

df['target'] = (df['next_close'] > df['close']).astype(int)
print(df['target'])

df = df.dropna()
print(df)

df['date'] = pd.to_datetime(df['date'])
print(df['date'])

df['day_of_week'] = df['date'].dt.dayofweek
print(df['day_of_week'])

df['day_of_month'] = df['date'].dt.day
print(df['day_of_month'])

df['daily_return'] = (df['close'] - df['open']) / df['open']
print(df['daily_return'])

df['volatility'] = (df['high'] - df['low']) / df['open']
print(df['volatility'])

features = ['open', 'high', 'low', 'close', 'volume', 'daily_return', 'volatility', 'day_of_week', 'day_of_month']
X = df[features]
y = df['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42)
}

results = {}
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    results[name] = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1': f1_score(y_test, y_pred)
    }

results_df = pd.DataFrame(results).T
print(results_df)

print(f"Best parameters: {models['Random Forest'].get_params()}")

y_pred = models['Random Forest'].predict(X_test_scaled)
print("\nRandom Forest Performance:")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(f"Precision: {precision_score(y_test, y_pred):.4f}")
print(f"Recall: {recall_score(y_test, y_pred):.4f}")
print(f"F1 Score: {f1_score(y_test, y_pred):.4f}")

feature_importance = models['Random Forest'].feature_importances_
features_df = pd.DataFrame({'Feature': features, 'Importance': feature_importance})
features_df = features_df.sort_values('Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=features_df)
plt.title('Feature Importance')
plt.tight_layout()
plt.show()

from google.colab import files
uploaded = files.upload()
for filename in uploaded.keys():
    print(f'Uploaded file: {filename} ({len(uploaded[filename])} bytes)')

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import string
#Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')
#Load data
df = pd.read_csv('3) Sentiment dataset.csv')
#Text cleaning function
def clean_text(text):
    #Convert to lowercase
    text = text.lower()
    #Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    #Tokenize
    tokens = word_tokenize(text)
    #Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    #Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)
#Apply cleaning to text column
df['Cleaned_Text'] = df['Text'].apply(clean_text)

from textblob import TextBlob
def get_sentiment(text):
    analysis = TextBlob(text)
    if analysis.sentiment.polarity > 0:
        return 'Positive'
    elif analysis.sentiment.polarity < 0:
        return 'Negative'
    else:
        return 'Neutral'
df['Predicted_Sentiment'] = df['Cleaned_Text'].apply(get_sentiment)
print(df['Predicted_Sentiment'])

from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(df['Sentiment'], df['Predicted_Sentiment']))
print(confusion_matrix(df['Sentiment'], df['Predicted_Sentiment']))

from wordcloud import WordCloud
import matplotlib.pyplot as plt
#for positive sentiments
positive_text = ' '.join(df[df['Predicted_Sentiment'] == 'Positive']['Cleaned_Text'])
wordcloud = WordCloud(width=800, height=400).generate(positive_text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('Positive Sentiment Word Cloud')
plt.axis('off')
plt.show()
#for negative sentiments
negative_text = ' '.join(df[df['Predicted_Sentiment'] == 'Negative']['Cleaned_Text'])
wordcloud = WordCloud(width=800, height=400).generate(negative_text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('Negative Sentiment Word Cloud')
plt.axis('off')
plt.show()